{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c83b4e-8b9c-4f55-83f3-216e283eda4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef1337-0776-48b7-944b-b5ebf1942bd0",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Text preprocessing is an essential step in natural language processing (NLP) tasks. It involves transforming raw text data into a format that is more suitable for analysis and machine learning algorithms. In this tutorial, we will cover various common techniques for text preprocessing. Let's dive in!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b099c7-21b4-493b-9258-cf1de0a4f855",
   "metadata": {},
   "source": [
    "### Lowercasing\n",
    "Converting all text to lowercase can help to normalize the data and reduce the vocabulary size. It ensures that words in different cases are treated as the same word. For example, \"apple\" and \"Apple\" will both be transformed to \"apple\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c5b5c99-a599-4482-aa6a-de162271d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Nepotism is the act of granting an [advantage], privilege, or position to relatives in an occupation or field.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9264230d-921c-45ce-ad1c-b804309e6f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nepotism is the act of granting an [advantage], privilege, or position to relatives in an occupation or field.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_sent = sent.lower()\n",
    "lower_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c7118a-926c-470a-ad48-6448e5028563",
   "metadata": {},
   "source": [
    "### Removal of Punctuation and Special Characters\n",
    "Punctuation marks and special characters often do not add much meaning to the text and can be safely removed. Common punctuation marks include periods, commas, question marks, and exclamation marks. You can use regular expressions or string operations to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40a06c58-ef02-4f9c-86ba-9ddbdb39d4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nepotism is the act of granting an advantage privilege or position to relatives in an occupation or field'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = \"\"\n",
    "punctuautions = [\".\",\",\",\":\",\";\",\"!\",\"?\",\"[\",\"]\",\"'\"]\n",
    "for char in lower_sent:\n",
    "    if char not in punctuautions:\n",
    "        result += char\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28bd48bb-1c8a-490f-83aa-5b48dad2790c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nepotism is the act of granting an advantage privilege or position to relatives in an occupation or field'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "cleaned = re.sub(r'[^\\w\\d\\s]', '', lower_sent)\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d7eb74-f100-4611-8546-91b47ea1f4aa",
   "metadata": {},
   "source": [
    "### Stop Word Removal:\n",
    "Stop words are commonly occurring words in a language, such as \"a,\" \"an,\" \"the,\" \"is,\" and \"in.\" These words provide little semantic value and can be removed to reduce noise in the data. Libraries like NLTK provide a list of predefined stop words for different languages.\n",
    "\n",
    "Before using the code make sure you downloaded all the stopwords uning the first shell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e6c24bc-ea6d-4a82-b8a8-edb130f01606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shailesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07fb7071-9175-4db5-870b-cb99650f3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c02bd5a-400d-410b-aea7-3b2dad0f89cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_eng = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4e5e013-54ac-441a-8671-2ba9ce9bbaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = [word for word in cleaned.split(\" \") if word not in stopwords_eng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "659513aa-f870-46c3-b18d-21c7e8cb0fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nepotism act granting advantage privilege position relatives occupation field'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned = \" \".join(cleaned)\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0c645f-015b-45a4-8892-955683d1d382",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down a piece of text into smaller units called tokens. These tokens can be words, subwords, or even characters, depending on the level of granularity desired. Tokenization is a fundamental step in text preprocessing and is crucial for various natural language processing (NLP) tasks, such as machine translation, sentiment analysis, and language generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3784cc2-b9de-477f-8e2a-94b9a8f5990f",
   "metadata": {},
   "source": [
    "### Word Tokenization\n",
    "\n",
    "Word tokenization is the most common form of tokenization, where the text is split into individual words. For example, given the sentence \"Tokenization is important for NLP tasks,\" the word tokens would be: [\"Tokenization\", \"is\", \"important\", \"for\", \"NLP\", \"tasks\"].\n",
    "\n",
    "Word tokenization is typically performed using whitespace as the delimiter. However, it's important to handle cases like punctuation marks, contractions, and hyphenated words correctly. For example, \"don't\" should be tokenized as [\"do\", \"n't\"] instead of [\"don\", \"'\", \"t\"].\n",
    "\n",
    "Libraries like NLTK, spaCy, and the tokenizers package provide ready-to-use word tokenization functions.\n",
    "\n",
    "\n",
    "Before running any of these tokenization techniques, make sure you have `punkt` downloaded. `punkt` refers to the Punkt Tokenizer, which is a pre-trained unsupervised machine learning model for sentence tokenization. The NLTK Punkt Tokenizer is trained on large corpora and is capable of handling a wide range of sentence boundary detection for multiple languages. It uses a combination of rule-based heuristics and statistical models to identify sentence boundaries accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9159c38-2b41-44f1-b148-f14c3529f3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/shailesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/shailesh/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0076b5ee-b563-46c2-a453-0de02c17b695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nepotism',\n",
       " 'act',\n",
       " 'granting',\n",
       " 'advantage',\n",
       " 'privilege',\n",
       " 'position',\n",
       " 'relatives',\n",
       " 'occupation',\n",
       " 'field']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(cleaned)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813be63f-fb7f-4469-8250-da2848699434",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Stemming and lemmatization are techniques used in natural language processing (NLP) to reduce words to their base or root forms. Both approaches aim to normalize words and reduce inflectional variations, enabling better analysis and comparison of words. However, they differ in their methods and outputs. Let's dive into each technique in detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da24f2e-6538-401d-82ae-9692f35af0f8",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is a process of reducing words to their base or root forms by removing prefixes or suffixes. The resulting form is often a stem, which may not be an actual word itself. The primary goal of stemming is to simplify the vocabulary and group together words with the same base meaning.\n",
    "\n",
    "For example, when using a stemming algorithm on the words \"running,\" \"runs,\" and \"ran,\" the common stem would be \"run.\" The stemming process cuts off the suffixes (\"-ning,\" \"-s,\" and \"-\"), leaving behind the core form of the word.\n",
    "\n",
    "Stemming algorithms follow simple rules and heuristics based on linguistic patterns, rather than considering the context or part of speech of the word. Some popular stemming algorithms include the Porter stemming algorithm, the Snowball stemmer (which supports multiple languages), and the Lancaster stemming algorithm.\n",
    "\n",
    "Stemming is a computationally lightweight approach and can be useful in certain cases where the exact word form is not crucial. However, it may produce stems that are not actual words, leading to potential loss of meaning and ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abd32a8c-dbf9-49d7-b94b-8073200b273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer, PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c461d6e0-d821-4f7a-a3f1-3adfc0074c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = LancasterStemmer()\n",
    "stemmer.stem(\"happily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4bb06f-8c45-46f4-99c1-0a08d7a28cba",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization, on the other hand, aims to reduce words to their canonical or dictionary forms, known as lemmas. Unlike stemming, lemmatization considers the context and part of speech (POS) of the word to generate meaningful lemmas. The resulting lemmas are actual words found in the language's dictionary.\n",
    "\n",
    "For example, when lemmatizing the words \"running,\" \"runs,\" and \"ran,\" the lemma for each would be \"run.\" Lemmatization takes into account the POS information to accurately determine the base form of the word.\n",
    "\n",
    "Lemmatization algorithms use linguistic rules and morphological analysis to identify the appropriate lemma. They often rely on language-specific resources, such as word lists and morphological databases. Some popular lemmatization tools include the WordNet lemmatizer and the spaCy library (which supports lemmatization for multiple languages).\n",
    "\n",
    "Lemmatization typically produces more accurate and meaningful results compared to stemming because it retains the core meaning of words. It is especially useful in tasks that require precise word analysis, such as information retrieval, question answering, and sentiment analysis.\n",
    "\n",
    "However, lemmatization can be more computationally intensive compared to stemming due to its reliance on POS tagging and language-specific resources.\n",
    "\n",
    "Before running any of these tokenization techniques, make sure you have `wordnet` downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ca16ff5-1bea-421a-871a-72d2ff57f605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/shailesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc07e5e3-9906-40bc-bc59-ba07120d5723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93b4656d-9839-4257-9d25-c04f1a6da989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tasty'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"tasty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cfeac141-9a5e-472e-a83d-a43e123f3674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e6a80-b3a4-44a3-9a8a-f05ab49cb563",
   "metadata": {},
   "source": [
    "When deciding between stemming and lemmatization, consider the trade-off between simplicity and accuracy. If you require speed and a broad reduction of word forms, stemming may be sufficient. However, if you need more accurate analysis and want to preserve the semantic meaning of words, lemmatization is generally the preferred choice.\n",
    "\n",
    "It's important to note that both stemming and lemmatization have limitations. They may not always produce the correct base forms, especially for irregular words or those not present in the chosen language's dictionary. Contextual information, such as word sense disambiguation, can further enhance the accuracy of both techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd939644-48fa-471a-82a7-4639729b1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de9f34b1-1e5f-4262-9c27-bad5f83422d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nepotism',\n",
       " 'act',\n",
       " 'granting',\n",
       " 'advantage',\n",
       " 'privilege',\n",
       " 'position',\n",
       " 'relative',\n",
       " 'occupation',\n",
       " 'field']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dceb229-8a0c-4b56-906b-78b852158048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
