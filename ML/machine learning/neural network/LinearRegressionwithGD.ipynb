{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "942a1a23-7fe3-4340-affb-6d14cdd32a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b720d80-cfa7-4d53-8d70-08b7fd0b6f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  radio  newspaper  sales\n",
       "1  230.1   37.8       69.2   22.1\n",
       "2   44.5   39.3       45.1   10.4\n",
       "3   17.2   45.9       69.3    9.3\n",
       "4  151.5   41.3       58.5   18.5\n",
       "5  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"https://www.statlearning.com/s/Advertising.csv\", index_col=0)\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b1508e-710f-4151-afbb-9caf0f4b074e",
   "metadata": {},
   "source": [
    "This is a multiple Linear Regression problem with three independent variables: TV , radio and newspaper and one dependent variable: sales. There are 200 samples in the dataset *ie.* $n = 200$\n",
    "\n",
    "This multiple linear regression problem can be represented in matrix form as:\n",
    "\n",
    "$$\\mathbf{\\hat{y}} = \\mathbf{X} \\boldsymbol{\\beta}$$\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "\\hat{y_1} \\\\ \n",
    "\\hat{y_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{y_{200}}\n",
    "\\end{bmatrix} =   \\begin{bmatrix}\n",
    "  1 & x_{1\\ 1} & x_{1\\ 2} & x_{1\\ 3} \\\\\n",
    "  1 & x_{2\\ 1} & x_{2\\ 2} & x_{2\\ 3} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  1 & x_{200\\ 1} & x_{200\\ 2} & x_{200\\ 3}\n",
    " \\end{bmatrix} \\times \\begin{bmatrix}\n",
    "\\beta_0 \\\\ \n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\beta_3\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "The predicted output for the samples can be computed as:\n",
    "\\begin{align*}\\hat{y_1} &= \\beta_0x_{1\\ 0}+ \\beta_1x_{1\\ 1} + \\beta_2x_{1\\ 2} + \\beta_3 x_{1\\ 3}\\\\\n",
    "\\hat{y_2} &= \\beta_0x_{2\\ 0}+ \\beta_2x_{2\\ 1} + \\beta_2x_{2\\ 2} + \\beta_3 x_{2\\ 3}\\\\\n",
    "\\hat{y_3} &= \\beta_0x_{3\\ 0}+ \\beta_1x_{3\\ 1} + \\beta_2x_{3\\ 2} + \\beta_3 x_{3\\ 3}\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "\\hat{y_{200}} &= \\beta_0x_{200\\ 0}+ \\beta_1x_{200\\ 1} + \\beta_2x_{200\\ 2} + \\beta_3 x_{200\\ 3}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Generalizing, for any $i^{th}$ sample, predicted output can be computed as:\n",
    "\n",
    "$$\\hat{y_i} = \\beta_0x_{i\\ 0}+ \\beta_1x_{i\\ 1} + \\beta_2x_{i\\ 2} + \\beta_3 x_{i\\ 3}$$\n",
    " where for all $i$ = $1$ to $n$, $x_{i0} =1$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "172158d9-409c-4d8b-b923-bb9d4121a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_df.drop('sales', axis=1).to_numpy()\n",
    "y = data_df[['sales']].to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08ee3cf0-4da8-476b-9366-fdc053442eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concat(\n",
    "    (np.ones((200,1)) , scaled_X),\n",
    "    axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62b434fb-cf70-426f-8b90-8f01780e5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "n,d = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9c9a3e0-1d1c-4ebb-941f-57b486506950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93deeb33-2c62-409b-915b-2afa1787e18d",
   "metadata": {},
   "source": [
    "## Random Initialization\n",
    "\n",
    "Let's initialize the values of parameters randomly. The function `initialize_beta` uses the [`numpy.random.randn`](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randn.html) function to initialize the parameters using the random values sampled from **standard normal distribution**. It returns an array of the shape $d\\times 1$ (where $d$ = no. of features) containing the initial values of the parameters. In our case $d=4$ (including the ones column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66e570d1-32c7-47f9-b688-8b13ba9b1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(d):\n",
    "    np.random.seed(90)\n",
    "    betas = np.random.randn(d,1)\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a14e624c-6482-4a16-9591-368243ab419b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.22448667]\n",
      " [-0.22752394]\n",
      " [-0.59139893]\n",
      " [-0.79922765]]\n"
     ]
    }
   ],
   "source": [
    "betas = initializer(4)\n",
    "print(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff69eacf-448e-4e53-90c8-e4997828399d",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "In OLS, you minimized the sum of squared error (SSE). Here you will be minimizing the cost function. The cost function $J(.)$ is nothing but the sum of squared error multiplied by $\\frac{1}{2}$ to make the derivation easier. You should know that multiplying the cost function with $\\frac{1}{2}$ only changes the value of the cost function but not the optimal parameters that minimize it.\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\beta_0, \\beta_1, \\beta_2, \\beta_3) &= \\frac{1}{2}\\sum_{i=1}^{n}(\\hat{y_{i}}-{y_{i}})^2 \\\\\n",
    "&= \\frac{1}{2}\\sum_{i=1}^{n}((\\beta_0x_{i0}+\\beta_1x_{i1} +\\beta_2x_{i2} + \\beta_3x_{i3})-y_{i})^2\n",
    "\\end{align*}\n",
    "\n",
    "The cost function can be written in matrix form as:\n",
    "\n",
    "$$J(\\boldsymbol{\\beta}) = \\frac{1}{2}\\ \\sum(\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y})^2$$\n",
    "\n",
    "\n",
    "*Note: We call $J$ as a function of only parameters $\\boldsymbol{\\beta}$ but not of $X$ and $y$ because $X$ and $y$ are constants given by the dataset. So the value of $J$ depends only on the parameters.*\n",
    "\n",
    "**You want to find the parameters $\\beta_0, \\beta_1, \\beta_2$ and $\\beta_3$ that minimizes the cost function $J$ using Gradient Descent.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "985956d2-bfcc-4e71-bb29-2c3aa512be38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(23877.15553686013)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_cost(betas):\n",
    "    cost = 0.5 * np.sum(np.square(np.dot(X, betas) - y))\n",
    "    return cost\n",
    "\n",
    "calculate_cost(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23960f3a-c511-4c2d-a810-6b1f1f23e06d",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "Partial derivative(gradient) of the cost function with respect to $\\beta_1$, \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J}{\\partial \\beta_1} &= \\frac{\\partial}{\\partial \\beta_1}\\ \\frac{1}{2}\\ \\sum_{i=1}^{n}(\\hat{y_{i}}-{y_{i}})^2\\\\\n",
    "&=\\frac{1}{2}\\ \\sum_{i=1}^{n}\\frac{\\partial}{\\partial \\beta_1}(\\hat{y_{i}}-{y_{i}})^2\n",
    "\\end{align*}\n",
    "\n",
    "$\\hspace{8cm}$Applying chain rule,\n",
    "\n",
    "\\begin{align*}\n",
    "\\hspace{8cm}&=\\frac{1}{2}\\ \\sum_{i=1}^{n}\\frac{\\partial (\\hat{y_{i}}-{y_{i}})^2}{\\partial (\\hat{y_{i}}-{y_{i}})} \\times \\frac{\\partial (\\hat{y_{i}}-{y_{i}})}{\\partial \\beta_1}\\\\\n",
    "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i}) \\times \\frac{\\partial (\\beta_0x_{i0} + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3})-y_i)}{\\partial \\beta_1}\\\\\n",
    "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i}) \\times x_{i1}\\\\\n",
    "\\therefore \\frac{\\partial J}{\\partial \\beta_1}&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i1} \n",
    "\\end{align*}\n",
    "\n",
    "Similarly, \n",
    "\n",
    "\\begin{align*}\\frac{\\partial J}{\\partial \\beta_0}&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i0}\\\\\n",
    "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})\\times 1\\\\\n",
    "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\beta_2}=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i2}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\beta_3}=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i3}$$\n",
    "\n",
    "In general, the formula for calculating the gradients with respect to a parameter $\\beta_j$ can be expressed as:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\beta_j}=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{ij}$$\n",
    "\n",
    "\n",
    "We can write this generalized expression in matrix form to calculate the gradients wrt. all the parameters simultaneously as:\n",
    "\n",
    "$$\\frac{\\boldsymbol{\\partial J}}{\\boldsymbol{\\partial \\beta}}= \\mathbf{X^T}(\\mathbf{\\hat{y}-y}) = \\begin{bmatrix}\n",
    "\\frac{\\partial J}{\\partial \\beta_0} \\\\ \n",
    "\\frac{\\partial J}{\\partial \\beta_1}\\\\\n",
    "\\frac{\\partial J}{\\partial \\beta_2}\\\\\n",
    "\\frac{\\partial J}{\\partial \\beta_3}\n",
    "\\end{bmatrix} $$`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09b47a73-bc33-414d-afd6-57e80ea28ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2849.39733364],\n",
       "       [ -875.24367879],\n",
       "       [ -777.153904  ],\n",
       "       [ -441.93833065]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_gradients(X, y, betas):\n",
    "    gradient = np.dot(X.T, (np.dot(X, betas) - y))\n",
    "    return gradient\n",
    "\n",
    "calculate_gradients(X,y, betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96694cd0-8873-4bbd-b044-80e5fcd0b54b",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Now you need to update the parameters using their respective gradients until the cost function converges to its minimum value.\n",
    "\n",
    "\n",
    "${\\hspace{5cm}}\\text{Repeat until convergence }\\{$\n",
    "\n",
    "$$\\beta_0 :=\\beta_0-\\alpha\\frac{\\partial J}{\\partial \\beta_0}$$\n",
    "\n",
    "$$\\beta_1 :=\\beta_1-\\alpha\\frac{\\partial J}{\\partial \\beta_1}$$\n",
    "\n",
    "$$\\beta_2 :=\\beta_2-\\alpha\\frac{\\partial J}{\\partial \\beta_2}$$\n",
    "\n",
    "$$\\beta_3 :=\\beta_3-\\alpha\\frac{\\partial J}{\\partial \\beta_3}$$\n",
    "\n",
    "${\\hspace{8cm}}\\}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Since you already have a vector $\\beta$ called `beta` containing parameters and an another vector $\\frac{\\partial J}{\\partial \\beta}$ called `gradients` containing the gradients of cost function with respect to the parameters, this updation is a simple matrix operation:\n",
    "\n",
    "$$\\boldsymbol{\\beta} := \\boldsymbol{\\beta} - \\alpha \\boldsymbol{\\frac{\\partial J}{\\partial \\beta}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc9138aa-2515-4f72-951b-19beb38db955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,y, alpha = 0.01, max_iter = 1000, change = 0.0001):\n",
    "\n",
    "    difference = 1\n",
    "    iteration = 0\n",
    "    n,d = X.shape\n",
    "    \n",
    "    # Forward\n",
    "    betas = initializer(d)\n",
    "    cost = calculate_cost(betas)\n",
    "    history = [cost]\n",
    "\n",
    "    while difference > change and iteration <= max_iter:\n",
    "        # Backward propogation\n",
    "        betas = betas - alpha * calculate_gradients(X,y,betas)\n",
    "        \n",
    "        cost = calculate_cost(betas)\n",
    "        history.append(cost)\n",
    "\n",
    "        difference = np.abs(history[iteration] - cost)\n",
    "        print(f\"Iteration {iteration}, cost {cost}\")\n",
    "        iteration += 1\n",
    "\n",
    "        if cost == np.inf:\n",
    "            print(\"Cost reached infinity, try smaller lr\")\n",
    "            break\n",
    "\n",
    "    return betas, iteration, history\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26b3f07d-86a6-4bcf-9959-4a0456aa2cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, cost 570.223660185404\n",
      "Iteration 1, cost 318.16189765574126\n",
      "Iteration 2, cost 283.8368584350954\n",
      "Iteration 3, cost 279.1532997580384\n",
      "Iteration 4, cost 278.51382786321966\n",
      "Iteration 5, cost 278.42646528195417\n",
      "Iteration 6, cost 278.41452352280885\n",
      "Iteration 7, cost 278.4128903519616\n",
      "Iteration 8, cost 278.4126668927779\n",
      "Iteration 9, cost 278.4126363047094\n"
     ]
    }
   ],
   "source": [
    "opt_betas, best_step, history = gradient_descent(X,y, alpha=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4bac2649-b256-44b7-9528-ebbdd13afbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.0225    ],\n",
       "       [ 3.9192152 ],\n",
       "       [ 2.79189485],\n",
       "       [-0.02262516]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e881ce-683c-4727-a848-3d98f4a0dc3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
