{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdff572",
   "metadata": {},
   "source": [
    "# ðŸ“š Book Recommendation System â€” Extended\n",
    "\n",
    "This notebook builds multiple recommenders and **evaluates** them using RMSE, Precision@K, Recall@K, Coverage, and Novelty.\n",
    "\n",
    "**Models included:**\n",
    "- Popularity-based\n",
    "- Item-based Collaborative Filtering\n",
    "- Simple User-based Collaborative Filtering (neighborhood aggregation)\n",
    "- Matrix Factorization (Surprise SVD & NMF)\n",
    "- Content-based (TF-IDF + linear_kernel)\n",
    "- Hybrid (weighted combination)\n",
    "\n",
    "**Evaluation & plots** included.\n",
    "\n",
    "> NOTE: This notebook expects `books.csv`, `users.csv`, `ratings.csv` to be present in the working directory. Adjust thresholds to fit your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96484982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surprise not installed or import failed: No module named 'surprise'\n",
      "Install with: pip install scikit-surprise or pip install surprise==0.1.6\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd, numpy as np, pickle, os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Surprise (matrix factorization)\n",
    "try:\n",
    "    from surprise import Dataset, Reader, SVD, NMF, accuracy, dump\n",
    "    from surprise.model_selection import train_test_split as surprise_train_test_split, GridSearchCV as SurpriseGridSearchCV\n",
    "except Exception as e:\n",
    "    print('Surprise not installed or import failed:', e)\n",
    "    print('Install with: pip install scikit-surprise or pip install surprise==0.1.6')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b9b55fd-7ef1-4736-8c75-5e5a78eeee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-surprise\n",
      "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [45 lines of output]\n",
      "  \n",
      "  Error compiling Cython file:\n",
      "  ------------------------------------------------------------\n",
      "  ...\n",
      "          self.avg_cltr_i = avg_cltr_i\n",
      "          self.avg_cocltr = avg_cocltr\n",
      "  \n",
      "          return self\n",
      "  \n",
      "      def compute_averages(self, np.ndarray[np.int_t] cltr_u,\n",
      "                                               ^\n",
      "  ------------------------------------------------------------\n",
      "  \n",
      "  surprise\\prediction_algorithms\\co_clustering.pyx:157:45: Invalid type.\n",
      "  Compiling surprise/similarities.pyx because it changed.\n",
      "  Compiling surprise/prediction_algorithms/matrix_factorization.pyx because it changed.\n",
      "  Compiling surprise/prediction_algorithms/optimize_baselines.pyx because it changed.\n",
      "  Compiling surprise/prediction_algorithms/slope_one.pyx because it changed.\n",
      "  Compiling surprise/prediction_algorithms/co_clustering.pyx because it changed.\n",
      "  [1/5] Cythonizing surprise/prediction_algorithms/co_clustering.pyx\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\pkhad\\AppData\\Roaming\\Python\\Python313\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 389, in <module>\n",
      "      main()\n",
      "      ~~~~^^\n",
      "    File \"C:\\Users\\pkhad\\AppData\\Roaming\\Python\\Python313\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 373, in main\n",
      "      json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "                               ~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\pkhad\\AppData\\Roaming\\Python\\Python313\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 143, in get_requires_for_build_wheel\n",
      "      return hook(config_settings)\n",
      "    File \"C:\\Users\\pkhad\\AppData\\Local\\Temp\\pip-build-env-7yuo0o39\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 331, in get_requires_for_build_wheel\n",
      "      return self._get_build_requires(config_settings, requirements=[])\n",
      "             ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    File \"C:\\Users\\pkhad\\AppData\\Local\\Temp\\pip-build-env-7yuo0o39\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 301, in _get_build_requires\n",
      "      self.run_setup()\n",
      "      ~~~~~~~~~~~~~~^^\n",
      "    File \"C:\\Users\\pkhad\\AppData\\Local\\Temp\\pip-build-env-7yuo0o39\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 317, in run_setup\n",
      "      exec(code, locals())\n",
      "      ~~~~^^^^^^^^^^^^^^^^\n",
      "    File \"<string>\", line 116, in <module>\n",
      "    File \"C:\\Users\\pkhad\\AppData\\Local\\Temp\\pip-build-env-7yuo0o39\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1159, in cythonize\n",
      "      cythonize_one(*args)\n",
      "      ~~~~~~~~~~~~~^^^^^^^\n",
      "    File \"C:\\Users\\pkhad\\AppData\\Local\\Temp\\pip-build-env-7yuo0o39\\overlay\\Lib\\site-packages\\Cython\\Build\\Dependencies.py\", line 1303, in cythonize_one\n",
      "      raise CompileError(None, pyx_file)\n",
      "  Cython.Compiler.Errors.CompileError: surprise/prediction_algorithms/co_clustering.pyx\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2833cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (adjust paths if needed)\n",
    "books = pd.read_csv('books.csv')\n",
    "users = pd.read_csv('users.csv')\n",
    "ratings = pd.read_csv('ratings.csv')\n",
    "\n",
    "print('books', books.shape, 'users', users.shape, 'ratings', ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6a2a99-4dde-4275-b32b-13cde80b0fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c6d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning & merge\n",
    "books['Year-Of-Publication'] = pd.to_numeric(books['Year-Of-Publication'], errors='coerce')\n",
    "books.loc[(books['Year-Of-Publication'] < 1500) | (books['Year-Of-Publication'] > 2025), 'Year-Of-Publication'] = None\n",
    "books['Book-Author'].fillna('Unknown', inplace=True)\n",
    "books['Publisher'].fillna('Unknown', inplace=True)\n",
    "\n",
    "ratings_with_name = ratings.merge(books, on='ISBN')\n",
    "print('merged shape:', ratings_with_name.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d423c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Popularity-based (Top N)\n",
    "num_rating_df = ratings_with_name.groupby('Book-Title').count()['Book-Rating'].reset_index().rename(columns={'Book-Rating':'num_ratings'})\n",
    "avg_rating_df = ratings_with_name.groupby('Book-Title')['Book-Rating'].mean().reset_index().rename(columns={'Book-Rating':'avg_ratings'})\n",
    "\n",
    "popular_df = num_rating_df.merge(avg_rating_df, on='Book-Title')\n",
    "popular_df = popular_df[popular_df['num_ratings']>250].sort_values('avg_ratings', ascending=False).head(50)\n",
    "popular_df = popular_df.merge(books, on='Book-Title').drop_duplicates('Book-Title')[['Book-Title','Book-Author','Image-URL-M','num_ratings','avg_ratings']]\n",
    "popular_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab6be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative filtering preparation (use thresholds to reduce sparsity)\n",
    "x = ratings_with_name.groupby('User-ID').count()['Book-Rating'] > 200\n",
    "book_readers = x[x].index\n",
    "filtered_rating = ratings_with_name[ratings_with_name['User-ID'].isin(book_readers)]\n",
    "\n",
    "y = filtered_rating.groupby('Book-Title').count()['Book-Rating'] > 50\n",
    "famous_books = y[y].index\n",
    "final_ratings = filtered_rating[filtered_rating['Book-Title'].isin(famous_books)]\n",
    "\n",
    "# pivot: books x users\n",
    "pt = final_ratings.pivot_table(index='Book-Title', columns='User-ID', values='Book-Rating')\n",
    "pt.fillna(0, inplace=True)\n",
    "print('pt shape:', pt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79904f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item-based similarity (cosine on rows of pt)\n",
    "item_similarity = cosine_similarity(pt)\n",
    "print('item_similarity shape:', item_similarity.shape)\n",
    "book_list = list(pt.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-based similarity (cosine on users)\n",
    "user_item = pt.T  # users x books\n",
    "user_similarity = cosine_similarity(user_item)\n",
    "print('user_similarity shape:', user_similarity.shape)\n",
    "user_list = list(user_item.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7370c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-based: TF-IDF (title + author) + linear kernel\n",
    "books_unique = books.drop_duplicates('Book-Title').reset_index(drop=True)\n",
    "books_unique['features'] = books_unique['Book-Title'].astype(str) + ' ' + books_unique['Book-Author'].astype(str)\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(books_unique['features'])\n",
    "content_similarity = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "title_to_idx = {title: idx for idx, title in enumerate(books_unique['Book-Title'].values)}\n",
    "print('books_unique:', books_unique.shape, 'content_similarity shape:', content_similarity.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1ab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendation functions\n",
    "\n",
    "def recommend_item_cf(book_name, topk=5):\n",
    "    if book_name not in pt.index:\n",
    "        return []\n",
    "    idx = pt.index.get_loc(book_name)\n",
    "    sims = list(enumerate(item_similarity[idx]))\n",
    "    sims = sorted(sims, key=lambda x: x[1], reverse=True)[1: topk+1]\n",
    "    recs = []\n",
    "    for i, score in sims:\n",
    "        title = pt.index[i]\n",
    "        tmp = books[books['Book-Title']==title].drop_duplicates('Book-Title').iloc[0]\n",
    "        recs.append((tmp['Book-Title'], tmp['Book-Author'], tmp.get('Image-URL-M','')))\n",
    "    return recs\n",
    "\n",
    "def recommend_user_cf(book_name, topk=5):\n",
    "    # aggregate ratings from users who rated the book, using user similarity\n",
    "    users_who_rated = final_ratings[final_ratings['Book-Title']==book_name]['User-ID'].unique()\n",
    "    if len(users_who_rated)==0:\n",
    "        return []\n",
    "    user_index_map = {uid: i for i, uid in enumerate(user_item.index)}\n",
    "    agg_scores = defaultdict(float)\n",
    "    agg_weights = defaultdict(float)\n",
    "    for uid in users_who_rated:\n",
    "        if uid not in user_index_map:\n",
    "            continue\n",
    "        uidx = user_index_map[uid]\n",
    "        sim_vec = user_similarity[uidx]\n",
    "        for other_idx, sim_score in enumerate(sim_vec):\n",
    "            if sim_score <= 0: continue\n",
    "            rated = user_item.iloc[other_idx]\n",
    "            for book_title, rating in rated[rated>0].items():\n",
    "                if book_title == book_name: continue\n",
    "                agg_scores[book_title] += sim_score * rating\n",
    "                agg_weights[book_title] += abs(sim_score)\n",
    "    pred_scores = []\n",
    "    for book_title in agg_scores:\n",
    "        if agg_weights[book_title]==0: continue\n",
    "        pred_scores.append((book_title, agg_scores[book_title]/agg_weights[book_title]))\n",
    "    pred_scores = sorted(pred_scores, key=lambda x: x[1], reverse=True)[:topk]\n",
    "    recs = []\n",
    "    for title, sc in pred_scores:\n",
    "        tmp = books[books['Book-Title']==title].drop_duplicates('Book-Title').iloc[0]\n",
    "        recs.append((tmp['Book-Title'], tmp['Book-Author'], tmp.get('Image-URL-M','')))\n",
    "    return recs\n",
    "\n",
    "def recommend_content(book_name, topk=5):\n",
    "    if book_name not in title_to_idx:\n",
    "        return []\n",
    "    idx = title_to_idx[book_name]\n",
    "    sims = list(enumerate(content_similarity[idx]))\n",
    "    sims = sorted(sims, key=lambda x: x[1], reverse=True)[1: topk+1]\n",
    "    recs = []\n",
    "    for i, sc in sims:\n",
    "        tmp = books_unique.iloc[i]\n",
    "        recs.append((tmp['Book-Title'], tmp['Book-Author'], tmp.get('Image-URL-M','')))\n",
    "    return recs\n",
    "\n",
    "def recommend_hybrid(book_name, topk=5, alpha=0.6):\n",
    "    n = len(books_unique)\n",
    "    item_scores = np.zeros(n)\n",
    "    content_scores = np.zeros(n)\n",
    "    if book_name in pt.index:\n",
    "        idx_item = pt.index.get_loc(book_name)\n",
    "        sims = item_similarity[idx_item]\n",
    "        for i_pt, score in enumerate(sims):\n",
    "            title = pt.index[i_pt]\n",
    "            if title in title_to_idx:\n",
    "                item_scores[title_to_idx[title]] = score\n",
    "    if book_name in title_to_idx:\n",
    "        content_scores = content_similarity[title_to_idx[book_name]]\n",
    "    final = alpha*item_scores + (1-alpha)*content_scores\n",
    "    top_idx = np.argsort(final)[::-1][1: topk+1]\n",
    "    recs = []\n",
    "    for idx in top_idx:\n",
    "        tmp = books_unique.iloc[idx]\n",
    "        recs.append((tmp['Book-Title'], tmp['Book-Author'], tmp.get('Image-URL-M','')))\n",
    "    return recs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d42ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test recommendations (example)\n",
    "sample = pt.index[0] if len(pt.index)>0 else books_unique['Book-Title'].iloc[0]\n",
    "print('sample:', sample)\n",
    "print('Item-CF:', recommend_item_cf(sample))\n",
    "print('User-CF:', recommend_user_cf(sample))\n",
    "print('Content:', recommend_content(sample))\n",
    "print('Hybrid:', recommend_hybrid(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5734ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surprise: train SVD and NMF on final_ratings (user-item interactions)\n",
    "from surprise import Dataset, Reader, SVD, NMF, accuracy\n",
    "from surprise.model_selection import train_test_split as s_train_test_split\n",
    "\n",
    "ratings_surprise = final_ratings[['User-ID','Book-Title','Book-Rating']].copy()\n",
    "reader = Reader(rating_scale=(1,10))\n",
    "data = Dataset.load_from_df(ratings_surprise[['User-ID','Book-Title','Book-Rating']], reader)\n",
    "trainset, testset = s_train_test_split(data, test_size=0.2)\n",
    "# Train SVD (default small factors)\n",
    "svd = SVD(n_factors=50, random_state=42)\n",
    "svd.fit(trainset)\n",
    "pred_svd = svd.test(testset)\n",
    "print('SVD RMSE:', accuracy.rmse(pred_svd))\n",
    "\n",
    "# Train NMF\n",
    "nmf = NMF(n_factors=15, random_state=42)\n",
    "nmf.fit(trainset)\n",
    "pred_nmf = nmf.test(testset)\n",
    "print('NMF RMSE:', accuracy.rmse(pred_nmf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37760458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation helpers: Precision@K, Recall@K, Coverage, Novelty\n",
    "def get_user_ground_truth(test_df, user):\n",
    "    return set(test_df[test_df['User-ID']==user]['Book-Title'].values)\n",
    "\n",
    "def recommend_for_user_itemcf(user_id, train_df, k=5):\n",
    "    # produce top-k recommendations for a user using item-CF (weighted by user's ratings)\n",
    "    user_ratings = train_df[train_df['User-ID']==user_id]\n",
    "    if user_ratings.empty:\n",
    "        return []\n",
    "    scores = defaultdict(float)\n",
    "    weights = defaultdict(float)\n",
    "    for _, row in user_ratings.iterrows():\n",
    "        rated_title = row['Book-Title']\n",
    "        rating = row['Book-Rating']\n",
    "        if rated_title not in pt.index:\n",
    "            continue\n",
    "        rated_idx = pt.index.get_loc(rated_title)\n",
    "        sims = item_similarity[rated_idx]\n",
    "        for j, sim in enumerate(sims):\n",
    "            candidate = pt.index[j]\n",
    "            if candidate in user_ratings['Book-Title'].values: continue\n",
    "            scores[candidate] += sim * rating\n",
    "            weights[candidate] += abs(sim)\n",
    "    preds = []\n",
    "    for b in scores:\n",
    "        if weights[b]>0:\n",
    "            preds.append((b, scores[b]/weights[b]))\n",
    "    preds = sorted(preds, key=lambda x: x[1], reverse=True)[:k]\n",
    "    return [p[0] for p in preds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c377b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(model_func, train_df, test_df, k=5, n_users=200):\n",
    "    users = test_df['User-ID'].unique()[:n_users]\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    recommended_items = set()\n",
    "    for user in users:\n",
    "        gt = get_user_ground_truth(test_df, user)\n",
    "        if len(gt)==0: continue\n",
    "        recs = model_func(user, train_df, k)\n",
    "        recommended_items.update(recs)\n",
    "        hit = len(set(recs) & gt)\n",
    "        precisions.append(hit / k)\n",
    "        recalls.append(hit / len(gt))\n",
    "    precision = np.mean(precisions) if precisions else 0\n",
    "    recall = np.mean(recalls) if recalls else 0\n",
    "    coverage = len(recommended_items) / len(pt.index)\n",
    "    # novelty: use popularity: avg log pop of recommended items (lower popularity -> higher novelty)\n",
    "    pop_count = final_ratings['Book-Title'].value_counts().to_dict()\n",
    "    avg_pop = np.mean([pop_count.get(b,0) for b in recommended_items]) if recommended_items else 0\n",
    "    novelty = -np.log1p(avg_pop)\n",
    "    return precision, recall, coverage, novelty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7557b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split on final_ratings for evaluation\n",
    "train_df, test_df = train_test_split(final_ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Evaluating Item-CF (top-5)...')\n",
    "prec_item, rec_item, cov_item, nov_item = precision_recall_at_k(recommend_for_user_itemcf, train_df, test_df, k=5, n_users=200)\n",
    "print('Item-CF -> Precision@5:', round(prec_item,4), 'Recall@5:', round(rec_item,4), 'Coverage:', round(cov_item,4), 'Novelty:', round(nov_item,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper for content-based recommendations for user: use one book from train as query\n",
    "def model_content_wrapper(user, train_df, k=5):\n",
    "    user_books = train_df[train_df['User-ID']==user]['Book-Title'].values\n",
    "    if len(user_books)==0: return []\n",
    "    query = user_books[0]\n",
    "    recs = recommend_content(query, topk=k)\n",
    "    return [r[0] for r in recs]\n",
    "\n",
    "print('Evaluating Content-Based (top-5)...')\n",
    "prec_cb, rec_cb, cov_cb, nov_cb = precision_recall_at_k(model_content_wrapper, train_df, test_df, k=5, n_users=200)\n",
    "print('Content -> Precision@5:', round(prec_cb,4), 'Recall@5:', round(rec_cb,4), 'Coverage:', round(cov_cb,4), 'Novelty:', round(nov_cb,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c23aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hybrid wrapper\n",
    "def model_hybrid_wrapper(user, train_df, k=5):\n",
    "    user_books = train_df[train_df['User-ID']==user]['Book-Title'].values\n",
    "    if len(user_books)==0: return []\n",
    "    query = user_books[0]\n",
    "    recs = recommend_hybrid(query, topk=k, alpha=0.6)\n",
    "    return [r[0] for r in recs]\n",
    "\n",
    "print('Evaluating Hybrid (top-5)...')\n",
    "prec_h, rec_h, cov_h, nov_h = precision_recall_at_k(model_hybrid_wrapper, train_df, test_df, k=5, n_users=200)\n",
    "print('Hybrid -> Precision@5:', round(prec_h,4), 'Recall@5:', round(rec_h,4), 'Coverage:', round(cov_h,4), 'Novelty:', round(nov_h,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ebe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision@5 and Recall@5 comparison\n",
    "methods = ['Item-CF','Content','Hybrid']\n",
    "precisions = [prec_item, prec_cb, prec_h]\n",
    "recalls = [rec_item, rec_cb, rec_h]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "rects1 = ax.bar(x - width/2, precisions, width, label='Precision@5')\n",
    "rects2 = ax.bar(x + width/2, recalls, width, label='Recall@5')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Comparison of Precision@5 and Recall@5')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods)\n",
    "ax.legend()\n",
    "for rect in rects1+rects2:\n",
    "    height = rect.get_height()\n",
    "    ax.annotate(f'{height:.3f}', xy=(rect.get_x()+rect.get_width()/2, height), xytext=(0,3), textcoords='offset points', ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save key artifacts for Flask\n",
    "pickle.dump(pt, open('pt.pkl','wb'))\n",
    "pickle.dump(books, open('books.pkl','wb'))\n",
    "pickle.dump(item_similarity, open('item_similarity.pkl','wb'))\n",
    "pickle.dump(content_similarity, open('content_similarity.pkl','wb'))\n",
    "pickle.dump(tfidf, open('tfidf.pkl','wb'))\n",
    "pickle.dump(books_unique, open('books_unique.pkl','wb'))\n",
    "print('Saved pickles: pt.pkl, books.pkl, item_similarity.pkl, content_similarity.pkl, tfidf.pkl, books_unique.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b39f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print('Precision@5: Item-CF', round(prec_item,4), 'Content', round(prec_cb,4), 'Hybrid', round(prec_h,4))\n",
    "print('RMSE: see SVD/NMF outputs earlier')\n",
    "print('Choose Hybrid if it balances precision and coverage; choose SVD/NMF for rating prediction accuracy (RMSE).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
