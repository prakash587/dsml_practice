{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f598e1b7-3818-4333-b3e5-ca228b9ed9d8",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares with Simple Linear Regression\n",
    "\n",
    "\n",
    "The simple linear regression model is:\n",
    "\n",
    " $$\\mathbf{y} = \\beta_0 +\\beta_1\\mathbf{x}$$\n",
    "\n",
    "\n",
    "where, we need to estimate the parameters, intercept($\\beta_0$) and slope($\\beta_1$). \n",
    "\n",
    "\n",
    "Let's recall an Advertising dataset and simple linear regression performed on scatter plot of _sales_ Vs. _TV_. With the help of `Scikit-Learn`, we were able to fit the best regression line among all the possibilities. Here is a snapshot:\n",
    "\n",
    "<figure align=\"center\">\n",
    "       <img src=\"./fig1.png\" height=\"350\" width=\"600\">\n",
    "       <figcaption>Figure 1: Simple Linear Regression </figcaption>\n",
    "   </figure>\n",
    "\n",
    "\n",
    "\n",
    "The blue line is a simple linear regression line with output $\\mathbf{y}$ as `sales` and $\\mathbf{x}$ as `TV`. The residual or error, $\\epsilon$ is the difference between the observed value, $y_i$, and predicted value, $\\hat{y_i}$. The observed value is the actual output data point, which is all blue dots in the figure, and the predicted value is the point given by the black regression line. Error for each output data point is shown by the vertical distance from the actual output data point to the predicted point on a regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c795154-f788-4d79-a213-deb6bd0b3646",
   "metadata": {},
   "source": [
    "The predicted output value is:\n",
    "\n",
    "$$\\hat{y_i} = \\beta_0 + \\beta_1x_i$$\n",
    "\n",
    "The observed (actual) output value is:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i$$\n",
    "\n",
    "Where $\\epsilon_i$ is a random error, not a parameter. The error $\\epsilon_i$ as ($y_{i}-\\hat{y_{i}}$) can either be positive or negative or even 0 sometimes. As we can see in the figure, vertical lines are on either side of the regression line. To avoid the cancellation of the error while summing errors, we square each error and sum them, called _Residual Sum of Squares (RSS)_ or _Sum of Squared Errors (SSE)_.\n",
    "\n",
    "$$\\text{Sum of Squared Errors (SSE)} = \\sum_{i=1}^{n}(y_{i}-\\hat{y_{i}})^2$$\n",
    "\n",
    "The summation is indexed from $1$ to $n$, since we have $n$ samples. Sum of Squared Errors (SSE) is the function of $\\beta_0$ and $\\beta_1$. We can also take it as _Loss function_. The main principle of Least Squares is that we should end up choosing intercept ($\\beta_0$) and slope ($\\beta_1$) such that the overall sum is minimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e334b-9944-4ab0-bfc0-a20270732502",
   "metadata": {},
   "source": [
    "\n",
    "Thus, to estimate the parameters, we minimize the sum of squared error. Sum of Squared Errors (SSE) can also be written as:\n",
    "\n",
    "$$\\text{SSE} = \\sum_{i=1}^{n}(y_{i}-\\hat{y_{i}})^2 =\\sum_{i=1}^{n}(y_{i}-(\\beta_0+\\beta_1x_i))^2 $$\n",
    "\n",
    "\n",
    "\n",
    "$\\hat{y_i}$ is replaced with the simple linear regression model equation. Since we tend to minimize $\\text{SSE}$, it is also called an objective function. Since the objective function, $\\text{SSE}$ is a squared term, it is always positive. If we plot objective function, it would be a convex graph facing upwards. \n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "       <img src=\"./fig2.png\" height=\"400\" width=\"500\">\n",
    "       <figcaption>Figure 2: Convex cost function </figcaption>\n",
    "   </figure>\n",
    "\n",
    "\n",
    "The parameters at a minimum point are obtained from calculus by setting the first derivative of the objective function to $0$. Gradient or slope is always $0$ at the minimum point. We have two unknown parameters, intercept ($\\beta_0$) and slope ($\\beta_1$) so, we will take the partial derivative of _SSE_ with respect to $\\beta_0$ and $\\beta_1$ separately. We will set both partial derivatives to 0 and solve for $\\beta_0$ and $\\beta_1$ separately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecfd74f-ce18-4623-abbd-efbd52abcf1e",
   "metadata": {},
   "source": [
    "Taking partial derivatives with respect to $\\beta_0$:\n",
    "\n",
    "$$\\frac{\\partial\\ \\text{SSE}}{\\partial \\beta_0}  = \\frac{\\partial }{\\partial \\beta_0}\\sum(y_i-(\\beta_0+\\beta_1x_i))^2$$\n",
    "\n",
    "Note that the derivative of the sum is the sum of the derivatives. So, we can take the derivative inside the summation.\n",
    "\n",
    "$$\\frac{\\partial }{\\partial \\beta_0}\\sum(y_i-(\\beta_0+\\beta_1x_i))^2 = \\sum\\frac{\\partial }{\\partial \\beta_0}(y_i-(\\beta_0+\\beta_1x_i))^2 $$\n",
    "\n",
    "Now, applying power rule and chain rule, we get:\n",
    "\n",
    "\n",
    "$$= \\sum2(y_i-(\\beta_0+\\beta_1x_i))(-1) $$\n",
    "\n",
    "$$=-2\\sum(y_i-(\\beta_0+\\beta_1x_i)) ......(1)$$\n",
    "\n",
    "\n",
    "\n",
    "Now, with respect to $\\beta_1$:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial\\ {\\text{SSE}} }{\\partial \\beta_1} = \\frac{\\partial }{\\partial \\beta_1}\\sum(y_i-(\\beta_0+\\beta_1x_i))^2$$\n",
    "\n",
    "Again, the derivative of the sum is the sum of the derivatives, So, we take the derivative inside the summation.\n",
    "\n",
    "$$\\frac{\\partial }{\\partial \\beta_1}\\sum(y_i-(\\beta_0+\\beta_1x_i))^2 = \\sum\\frac{\\partial }{\\partial \\beta_1}(y_i-(\\beta_0+\\beta_1x_i))^2 $$\n",
    "\n",
    "\n",
    "Applying power rule, $2$ comes out front and exponent becomes $1$. We will also apply chain rule to encounter the coefficient of $\\beta_1$. \n",
    "$$= \\sum2(y_i-(\\beta_0+\\beta_1x_i))(-x_i) $$\n",
    "\n",
    "Cleaning up a bit, \n",
    "\n",
    "$$= -2\\sum x_i(y_i-(\\beta_0+\\beta_1x_i)) ......(2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb06b02-acf7-4ffd-918e-5ad5f7c01447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
